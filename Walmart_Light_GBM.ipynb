{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_ePHlcDK151"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error as rmse\n",
        "from lightgbm import LGBMRegressor\n",
        "import pickle\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import reciprocal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/Shared drives/Predictive Analysis- Walmart'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzvM-HBKK3Os",
        "outputId": "a03b680b-a451-4932-f735-d1ff43e89701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/Shared drives/Predictive Analysis- Walmart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('./processed_data_by_store/CA_1.csv')\n",
        "# data[data[\"id_\"]==2778].to_csv(\"walmart_sample2778.csv\",index=False)\n",
        "# data[data[\"id_\"]==2778].columns"
      ],
      "metadata": {
        "id": "jDbV5fbPSJ_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg = data[['demand', 'day']].groupby('day').sum()"
      ],
      "metadata": {
        "id": "ENnPr1Ton2p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg.replace({0:np.NaN}).interpolate().to_csv(\"walmart_CA_1_demand.csv\",index=False)"
      ],
      "metadata": {
        "id": "vWcHeuCypMpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_train_evaluation_ = pd.read_csv('./input/sales_train_evaluation.csv')\n",
        "calendar_ = pd.read_csv('./input/calendar.csv')\n",
        "sell_prices_ = pd.read_csv('./input/sell_prices.csv')"
      ],
      "metadata": {
        "id": "9ur_vuMeLB_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "metadata": {
        "id": "hMzMzjI18F7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = sales_train_evaluation_[sales_train_evaluation_['store_id']=='CA_1']\n",
        "# calendar_.tail()\n",
        "# sell_prices_.tail()\n",
        "df=pd.melt(df,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],var_name='d',value_name='demand')\n",
        "# Merge calendar and sell_prices data\n",
        "df=pd.merge(df,calendar_,on='d',how='left')\n",
        "df=pd.merge(df,sell_prices_,on=['item_id','store_id','wm_yr_wk'],how='left')\n",
        "# df['sell_price']=df.groupby(['id'])['sell_price'].apply(lambda x: x.fillna(x.mean()))\n"
      ],
      "metadata": {
        "id": "FpDm-umnkoq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
        "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
        "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ftAzi-95lyHR",
        "outputId": "565c0df4-b1f3-4a53-cad3-60fefaa0c44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  store_id        item_id  wm_yr_wk  sell_price\n",
              "0     CA_1  HOBBIES_1_001     11325        9.58\n",
              "1     CA_1  HOBBIES_1_001     11326        9.58\n",
              "2     CA_1  HOBBIES_1_001     11327        8.26\n",
              "3     CA_1  HOBBIES_1_001     11328        8.26\n",
              "4     CA_1  HOBBIES_1_001     11329        8.26"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5a6670a-0810-4c3a-9ed5-8cef90fbefe7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>store_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>wm_yr_wk</th>\n",
              "      <th>sell_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CA_1</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>11325</td>\n",
              "      <td>9.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CA_1</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>11326</td>\n",
              "      <td>9.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CA_1</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>11327</td>\n",
              "      <td>8.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CA_1</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>11328</td>\n",
              "      <td>8.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CA_1</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>11329</td>\n",
              "      <td>8.26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5a6670a-0810-4c3a-9ed5-8cef90fbefe7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5a6670a-0810-4c3a-9ed5-8cef90fbefe7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5a6670a-0810-4c3a-9ed5-8cef90fbefe7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_processing(x):\n",
        "  # Adding columns for the days d_1942 to d_1969 as nan for which we need to forecast sales\n",
        "  for i in range(1942,1970):\n",
        "      x['d_'+str(i)]=np.nan\n",
        "      x['d_'+str(i)]=x['d_'+str(i)].astype(np.float16)\n",
        "\n",
        "  # Melting to convert one date to one observation\n",
        "  df=pd.melt(x,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],var_name='d',value_name='demand')\n",
        "  # Merge calendar and sell_prices data\n",
        "  df=pd.merge(df,calendar_,on='d',how='left')\n",
        "  df=pd.merge(df,sell_prices_,on=['item_id','store_id','wm_yr_wk'],how='left')\n",
        "  df['sell_price']=df.groupby(['id'])['sell_price'].apply(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "  # Fill events N/A\n",
        "  cat=['event_name_1','event_type_1','event_name_2','event_type_2']\n",
        "  for i in cat:\n",
        "      df[i].fillna('no_event',inplace=True)\n",
        "\n",
        "  # Create is_weekend feature\n",
        "  f=lambda x: 1 if x<=2 else 0\n",
        "  df['is_weekend']=df['wday'].map(f)\n",
        "  df['is_weekend']=df['is_weekend'].astype(np.int8)\n",
        "\n",
        "  # Create month_day feature\n",
        "  f=lambda x: x.split(\"-\")[2]\n",
        "  df['month_day']=df['date'].map(f)\n",
        "  df['month_day']=df['month_day'].astype(np.int8)\n",
        "\n",
        "  # Create month_week_number feature\n",
        "  df['month_week_number']=(df['month_day']-1) // 7 + 1  \n",
        "  df['month_week_number']=df['month_week_number'].astype(np.int8)\n",
        "\n",
        "  # Get sales by 3 aggregation level: store_id, item_id, store_id+dept_id\n",
        "  store_demand = df.groupby(['store_id','d'])['demand'].sum().reset_index().rename(columns={'demand':'store_demand'})\n",
        "  df = pd.merge(df, store_demand, on=['store_id','d'],how='left')\n",
        "  del store_demand\n",
        "\n",
        "  item_demand = df.groupby(['item_id','d'])['demand'].sum().reset_index().rename(columns={'demand':'item_demand'})\n",
        "  df = pd.merge(df, item_demand, on=['item_id','d'],how='left')\n",
        "  del item_demand\n",
        "\n",
        "  store_dept_demand = df.groupby(['store_id','dept_id','d'])['demand'].sum().reset_index().rename(columns={'demand':'store_dept_demand'})\n",
        "  df = pd.merge(df, store_dept_demand, on=['store_id','dept_id','d'],how='left')\n",
        "  del store_dept_demand\n",
        "\n",
        "  # Create lag variables\n",
        "  lag_col_prefixs = ['lag_','lag_price_','lag_store_demand_','lag_item_demand_','lag_store_dept_demand_']\n",
        "  lag_col_names = ['demand','sell_price','store_demand','item_demand','store_dept_demand']\n",
        "  for col_prefix, col_name in zip(lag_col_prefixs, lag_col_names):\n",
        "    lags=[28,35,42]\n",
        "    lag_cols=[]\n",
        "    for i in lags:\n",
        "        df[col_prefix+str(i)]=df.groupby(['id'])[col_name].shift(i)\n",
        "        lag_cols.append(col_prefix+str(i))\n",
        "    # lag_cols=['lag_28','lag_35','lag_42']\n",
        "    for i in lag_cols:\n",
        "        df[i].fillna(0,inplace=True)\n",
        "\n",
        "  # Encoding\n",
        "  labelencoder=LabelEncoder() \n",
        "  category=['event_name_1','event_type_1','event_name_2','event_type_2','id','item_id','dept_id','cat_id','store_id','state_id']\n",
        "  for i in category:\n",
        "      df[i+'_']=labelencoder.fit_transform(df[i])\n",
        "  df=df.drop(['event_name_1','event_type_1','event_name_2','event_type_2','id','item_id','dept_id','cat_id','store_id','state_id'],axis=1)\n",
        "\n",
        "  # Convert string date to int date\n",
        "  f=lambda x: x.split('_')[1]\n",
        "  df['day']=df['d'].map(f)\n",
        "  df['day']=df['day'].astype(np.int16) \n",
        "\n",
        "  # Cleanup - drop redundant columns\n",
        "  df=df.drop(['d','date','weekday','store_demand','item_demand','store_dept_demand'],axis=1)\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "DcXgbTuv8n7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ Data Processing ############\n",
        "# Calculate event count each day\n",
        "calendar_['event_count'] = calendar_[['event_name_1','event_name_2']].apply(lambda x: 2-x.isna().sum(), axis=1)\n",
        "# Apply data_processing function\n",
        "sales_train_evaluation_ = reduce_mem_usage(sales_train_evaluation_)\n",
        "processed_df = data_processing(sales_train_evaluation_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU5KT5UjDnQa",
        "outputId": "48f1b65e-5495-46a7-f9c5-83b58f01a831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 96.13 Mb (78.8% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ Hyperparameter tuning with RandomizedSearchCV ############\n",
        "def custom_split(X, y, groups):\n",
        "  for train_index, test_index in groups:\n",
        "    original_train_index = np.array(X[X['day'].isin(train_index+1)].index)\n",
        "    original_test_index = np.array(X[X['day'].isin(test_index+1)].index)\n",
        "    yield original_train_index, original_test_index\n",
        "\n",
        "# Train by store\n",
        "STORES_IDS = list(processed_df['store_id_'].unique())\n",
        "for store_id in STORES_IDS:\n",
        "  df1 = processed_df[processed_df.store_id_ == store_id]\n",
        "  # file_path = 'processed_data_by_store/'+store_id+'.csv'\n",
        "  # df1 = pd.read_csv(file_path)\n",
        "  df1 = reduce_mem_usage(df1)\n",
        "  # remove testing data\n",
        "  df1 = df1[df1['day']<1942]\n",
        "\n",
        "  df1 = df1.reset_index().drop(\"index\",axis=1)\n",
        "  X = df1.iloc[:, 1:]\n",
        "  y = df1['demand']\n",
        "  \n",
        "  tscv = TimeSeriesSplit(n_splits=5, test_size=28)\n",
        "  groups = tscv.split(df1['day'].unique())\n",
        "\n",
        "  lgb = LGBMRegressor(objective=\"tweedie\")\n",
        "\n",
        "  lgb_grid = {'learning_rate': reciprocal(3e-3, 3e-1),\n",
        "            'max_depth': list(range(50,70)) ,\n",
        "            'num_leaves': list(range(150,300)) ,\n",
        "            'n_estimators': list(range(100,200))}\n",
        "\n",
        "  lgb_reg = RandomizedSearchCV(lgb, param_distributions=lgb_grid,\n",
        "                            n_jobs=-1, scoring = 'neg_mean_squared_error', cv=custom_split(X, y, groups))\n",
        "  lgb_reg.fit(X, y)\n",
        "  print(store_id, '\\t', lgb_reg.best_params_)\n",
        "  print(store_id, '\\t', lgb_reg.best_score_)\n",
        "  model_path = 'lgb_model_dump/lgb_model_store_'+str(store_id)+'.bin'\n",
        "  pickle.dump(lgb_reg.best_estimator_, open(model_path, 'wb'))\n"
      ],
      "metadata": {
        "id": "rGYMRv5TCimz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize submission dataframe\n",
        "pred_test=pd.DataFrame()\n",
        "pred_test['id']=sales_train_evaluation_['id']\n",
        "pred_test['store_id']=sales_train_evaluation_['store_id'] \n",
        "for i in range(1,29):\n",
        "    pred_test['F'+str(i)]=np.nan\n",
        "    pred_test['F'+str(i)]=pred_test['F'+str(i)].astype(np.float16)\n",
        "    \n",
        "labelencoder=LabelEncoder() \n",
        "pred_test['store_id_']=labelencoder.fit_transform(pred_test['store_id'])"
      ],
      "metadata": {
        "id": "gauFBWe-2yya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make test predictions by store\n",
        "for store_id in STORES_IDS:\n",
        "  df = processed_df[processed_df.store_id_ == store_id]\n",
        "  # file_path = 'processed_data_by_store/'+store_id+'.csv'\n",
        "  # df = pd.read_csv(file_path)\n",
        "  x_test=df.loc[df['day']>=1942]\n",
        "  x_test = x_test.drop(['demand'],axis=1)\n",
        "  model_path = 'lgb_model_dump/lgb_model_store_'+str(store_id)+'.bin'\n",
        "  lgb = pickle.load(open(model_path, 'rb'))\n",
        "  k=1\n",
        "  for i in range(1942,1970):\n",
        "    # Read all our models and make predictions for each day/store pairs\n",
        "    pred_test['F'+str(k)][pred_test['store_id_']==store_id]=lgb.predict(x_test[x_test['day']==(i)]) \n",
        "    k+=1\n",
        "    \n",
        "prediction_test = np.round(pred_test,2) "
      ],
      "metadata": {
        "id": "FG6SOX0n9dgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the final submission file\n",
        "import time \n",
        "current_timestamp = int(time.time())\n",
        "prediction_test = prediction_test.drop(['store_id','store_id_'],axis=1)\n",
        "sample_submission = pd.read_csv('input/sample_submission.csv')\n",
        "sample_validation = sample_submission.iloc[:30490,:]\n",
        "final = pd.concat([sample_validation, prediction_test])\n",
        "file_path = 'lgb_submission/prediction_result' + str(current_timestamp) + '.csv'\n",
        "final.to_csv(file_path,index=False)"
      ],
      "metadata": {
        "id": "HVToqrKzITlU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}