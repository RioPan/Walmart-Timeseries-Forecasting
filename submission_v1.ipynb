{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V_ePHlcDK151"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from lightgbm import LGBMRegressor\n",
        "import pickle\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))"
      ],
      "metadata": {
        "id": "wgxyy0h46hF0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "# %cd '/content/drive/Shared drives/Predictive Analysis- Walmart'"
      ],
      "metadata": {
        "id": "hzvM-HBKK3Os"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_train_evaluation_ = pd.read_csv('/content/sales_train_evaluation.csv')\n",
        "calendar_ = pd.read_csv('/content/calendar.csv')\n",
        "sell_prices_ = pd.read_csv('/content/sell_prices.csv')\n",
        "STORES_IDS = list(sales_train_evaluation_['store_id'].unique())"
      ],
      "metadata": {
        "id": "9ur_vuMeLB_s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "metadata": {
        "id": "hMzMzjI18F7f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_processing(x):\n",
        "  # #Adding columns for the days d_1942 to d_1969 as nan for which we need to forecast sales\n",
        "  for i in range(1942,1970):\n",
        "      x['d_'+str(i)]=np.nan\n",
        "      x['d_'+str(i)]=x['d_'+str(i)].astype(np.float16)\n",
        "\n",
        "  # Melting to convert one date to one observation\n",
        "  df=pd.melt(x,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],var_name='d',value_name='demand')\n",
        "  # Merge calendar and sell_prices data\n",
        "  df=pd.merge(df,calendar_,on='d',how='left')\n",
        "  df=pd.merge(df,sell_prices_,on=['item_id','store_id','wm_yr_wk'],how='left')\n",
        "  df['sell_price']=df.groupby(['id'])['sell_price'].apply(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "  # Fill events N/A\n",
        "  cat=['event_name_1','event_type_1','event_name_2','event_type_2']\n",
        "  for i in cat:\n",
        "      df[i].fillna('no_event',inplace=True)\n",
        "\n",
        "  # Create is_weekend feature\n",
        "  f=lambda x: 1 if x<=2 else 0\n",
        "  df['is_weekend']=df['wday'].map(f)\n",
        "  df['is_weekend']=df['is_weekend'].astype(np.int8)\n",
        "\n",
        "  # Create month_day feature\n",
        "  f=lambda x: x.split(\"-\")[2]\n",
        "  df['month_day']=df['date'].map(f)\n",
        "  df['month_day']=df['month_day'].astype(np.int8)\n",
        "\n",
        "  # Create month_week_number feature\n",
        "  df['month_week_number']=(df['month_day']-1) // 7 + 1  \n",
        "  df['month_week_number']=df['month_week_number'].astype(np.int8)\n",
        "\n",
        "  # Lags\n",
        "  lags=[28,35,42]\n",
        "  for i in lags:\n",
        "      df['lag_'+str(i)]=df.groupby(['id'])['demand'].shift(i)\n",
        "  df = df.fillna(0)\n",
        "  # # Rolling Median\n",
        "  # window=[7,14,28,35,42]\n",
        "  # for i in window:\n",
        "  #     df['rolling_median_'+str(i)]=df.groupby(['id'])['demand'].transform(lambda s: s.rolling(i,center=False).median())\n",
        "\n",
        "  # window=['rolling_median_7','rolling_median_14','rolling_median_28','rolling_median_35','rolling_median_42']\n",
        "  # for i in window:\n",
        "  #     df[i]=df[i].fillna(0) \n",
        "\n",
        "  # Encoding\n",
        "  labelencoder=LabelEncoder() \n",
        "  category=['event_name_1','event_type_1','event_name_2','event_type_2','id','item_id','dept_id','cat_id','store_id','state_id']\n",
        "  for i in category:\n",
        "      df[i+'_']=labelencoder.fit_transform(df[i])\n",
        "\n",
        "  df=df.drop(['event_name_1','event_type_1','event_name_2','event_type_2','id','item_id','dept_id','cat_id','store_id','state_id'],axis=1)\n",
        "\n",
        "  # Convert string date to int date\n",
        "  f=lambda x: x.split('_')[1]\n",
        "  df['day']=df['d'].map(f)\n",
        "  df['day']=df['day'].astype(np.int16) \n",
        "\n",
        "  # Cleanup - drop redundant columns\n",
        "  df=df.drop(['d','date','weekday'],axis=1)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "DcXgbTuv8n7_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data by store, write to disk for further steps\n",
        "sales_train_evaluation_ = reduce_mem_usage(sales_train_evaluation_)\n",
        "for store_id in STORES_IDS:\n",
        "  x = sales_train_evaluation_[sales_train_evaluation_.store_id == store_id]\n",
        "  df = data_processing(x)\n",
        "  file_path = '/content/'+store_id+'.csv'\n",
        "  df.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU5KT5UjDnQa",
        "outputId": "5a4c8239-a755-4f6e-94fa-96dc42e08484"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 96.13 Mb (0.0% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "def tweedie_eval(y_pred, y_true, p=1.5):\n",
        "    y_true = y_true.get_label()\n",
        "    a = y_true*np.exp(y_pred, (1-p)) / (1-p)\n",
        "    b = np.exp(y_pred, (2-p))/(2-p)\n",
        "    loss = -a + b\n",
        "    return loss \n",
        "\n",
        "\n",
        "def custom_split(X, y, groups):\n",
        "  for train_index, test_index in groups:\n",
        "    original_train_index = np.array(X[X['day'].isin(train_index+1)].index)\n",
        "    original_test_index = np.array(X[X['day'].isin(test_index+1)].index)\n",
        "    yield original_train_index, original_test_index\n",
        "\n",
        "tweedie = make_scorer(tweedie_eval)"
      ],
      "metadata": {
        "id": "bz6PIG757UMQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning with RandomizedSearchCV\n",
        "from scipy.stats import reciprocal\n",
        "\n",
        "for store_id in STORES_IDS:\n",
        "  file_path = '/content/'+store_id+'.csv'\n",
        "  df1 = pd.read_csv(file_path)\n",
        "  df1 = reduce_mem_usage(df1)\n",
        "  # remove testing data\n",
        "  df1 = df1[df1['day']<1942]\n",
        "\n",
        "  df = df1.reset_index().drop(\"index\",axis=1)\n",
        "  X = df.iloc[:, 1:]\n",
        "  y = df['demand']\n",
        "  \n",
        "  tscv = TimeSeriesSplit(n_splits=5, test_size=28)\n",
        "  groups = tscv.split(df1['day'].unique())\n",
        "\n",
        "  lgb = LGBMRegressor( objective='tweedie')\n",
        "\n",
        "  lgb_grid = {\n",
        "      'tweedie_variance_power': [1.1],\n",
        "      'learning_rate': reciprocal(3e-3, 3e-1),\n",
        "            'max_depth': list(range(50,70)) ,\n",
        "            'n_estimators': list(range(100,300)),\n",
        "            'num_leaves': list(range(150,300)) }\n",
        "\n",
        "  lgb_reg = RandomizedSearchCV(lgb, param_distributions=lgb_grid,\n",
        "                            n_jobs=-1, scoring ='neg_root_mean_squared_error', cv=custom_split(X, y, groups))\n",
        "  lgb_reg.fit(X, y)\n",
        "  print(store_id, '\\t', lgb_reg.best_params_,lgb_reg.best_score_)\n",
        "  model_path = '/content/lgb_model_'+store_id+'.bin'\n",
        "  pickle.dump(lgb_reg.best_estimator_, open(model_path, 'wb'))\n"
      ],
      "metadata": {
        "id": "rGYMRv5TCimz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2971bbaf-ccd2-4783-c2c9-7610f74ac3a3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe475194c50>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe475c00a10>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CA_1 \t {'learning_rate': 0.050610394940121636, 'max_depth': 67, 'n_estimators': 280, 'num_leaves': 194, 'tweedie_variance_power': 1.1} -2.30774943648447\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe475194750>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe476381490>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CA_2 \t {'learning_rate': 0.051583904830467577, 'max_depth': 53, 'n_estimators': 280, 'num_leaves': 222, 'tweedie_variance_power': 1.1} -2.144873011113553\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe476573050>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe476913d50>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CA_3 \t {'learning_rate': 0.07582967729206709, 'max_depth': 54, 'n_estimators': 270, 'num_leaves': 208, 'tweedie_variance_power': 1.1} -3.124439480568764\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe475194950>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe47593a8d0>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CA_4 \t {'learning_rate': 0.1478995769975061, 'max_depth': 62, 'n_estimators': 130, 'num_leaves': 299, 'tweedie_variance_power': 1.1} -1.4947936396158847\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe476492850>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe476821390>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TX_1 \t {'learning_rate': 0.16804816033695238, 'max_depth': 53, 'n_estimators': 150, 'num_leaves': 192, 'tweedie_variance_power': 1.1} -1.9312170398919613\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe475194350>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe47655e490>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TX_2 \t {'learning_rate': 0.036646621908299024, 'max_depth': 50, 'n_estimators': 270, 'num_leaves': 257, 'tweedie_variance_power': 1.1} -2.184581615599202\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe4634cded0>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe4752718d0>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TX_3 \t {'learning_rate': 0.16132262516317375, 'max_depth': 59, 'n_estimators': 240, 'num_leaves': 154, 'tweedie_variance_power': 1.1} -2.1470173603077294\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe475194e50>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe476459ad0>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WI_1 \t {'learning_rate': 0.04500479865741088, 'max_depth': 64, 'n_estimators': 220, 'num_leaves': 225, 'tweedie_variance_power': 1.1} -1.8347939897611092\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe475194650>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe474e74310>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WI_2 \t {'learning_rate': 0.0612933414734137, 'max_depth': 50, 'n_estimators': 290, 'num_leaves': 200, 'tweedie_variance_power': 1.1} -3.285028287799137\n",
            "Mem. usage decreased to 206.11 Mb (82.7% reduction)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=<generator object custom_split at 0x7fe475194050>,\n",
              "                   estimator=LGBMRegressor(objective='tweedie'), n_jobs=-1,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe475608110>,\n",
              "                                        'max_depth': [50, 51, 52, 53, 54, 55,\n",
              "                                                      56, 57, 58, 59, 60, 61,\n",
              "                                                      62, 63, 64, 65, 66, 67,\n",
              "                                                      68, 69],\n",
              "                                        'n_estimators': [100, 110, 120, 130,\n",
              "                                                         140, 150, 160, 170,\n",
              "                                                         180, 190, 200, 210,\n",
              "                                                         220, 230, 240, 250,\n",
              "                                                         260, 270, 280, 290],\n",
              "                                        'num_leaves': [150, 151, 152, 153, 154,\n",
              "                                                       155, 156, 157, 158, 159,\n",
              "                                                       160, 161, 162, 163, 164,\n",
              "                                                       165, 166, 167, 168, 169,\n",
              "                                                       170, 171, 172, 173, 174,\n",
              "                                                       175, 176, 177, 178, 179, ...],\n",
              "                                        'tweedie_variance_power': [1.1]},\n",
              "                   scoring='neg_root_mean_squared_error')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WI_3 \t {'learning_rate': 0.05896493503811511, 'max_depth': 65, 'n_estimators': 160, 'num_leaves': 203, 'tweedie_variance_power': 1.1} -2.352375351361978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize submission dataframe\n",
        "pred_test=pd.DataFrame()\n",
        "pred_test['id']=sales_train_evaluation_['id']\n",
        "pred_test['store_id']=sales_train_evaluation_['store_id'] \n",
        "for i in range(1,29):\n",
        "    pred_test['F'+str(i)]=np.nan\n",
        "    pred_test['F'+str(i)]=pred_test['F'+str(i)].astype(np.float16)"
      ],
      "metadata": {
        "id": "gauFBWe-2yya"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make test predictions by store\n",
        "for store_id in STORES_IDS:\n",
        "  file_path = '/content/'+store_id+'.csv'\n",
        "  df = pd.read_csv(file_path)\n",
        "  x_test=df.loc[df['day']>=1942]\n",
        "  x_test = x_test.drop(['demand'],axis=1)\n",
        "  model_path = '/content/lgb_model_'+store_id+'.bin'\n",
        "  lgb = pickle.load(open(model_path, 'rb'))\n",
        "  k=1\n",
        "  for i in range(1942,1970):\n",
        "    # Read all our models and make predictions for each day/store pairs\n",
        "    pred_test['F'+str(k)][pred_test['store_id']==store_id]=lgb.predict(x_test[x_test['day']==(i)]) \n",
        "    k+=1\n",
        "    \n",
        "prediction_test = np.round(pred_test,2) "
      ],
      "metadata": {
        "id": "FG6SOX0n9dgd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the final submission file\n",
        "import time \n",
        "current_timestamp = int(time.time())\n",
        "prediction_test = prediction_test.drop('store_id',axis=1)\n",
        "sample_submission = pd.read_csv('/content/sample_submission.csv')\n",
        "sample_validation = sample_submission.iloc[:30490,:]\n",
        "final = pd.concat([sample_validation, prediction_test])\n",
        "file_path = '/content/prediction_result' + str(current_timestamp) + '.csv'\n",
        "final.to_csv(file_path,index=False)"
      ],
      "metadata": {
        "id": "HVToqrKzITlU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ########################### TEST Predict ############################################\n",
        "\n",
        "# df=df.drop(['demand'],axis=1)\n",
        "\n",
        "# #Loading Already Trained LightGBM Regressor Model for Computaion \n",
        "# with open('/content/lgb_model.pkl','rb') as f:\n",
        "#     lgb=pickle.load(f)\n",
        "\n",
        "# pred_test=pd.DataFrame()\n",
        "# pred_test['id']=x['id'] \n",
        "# j=1\n",
        "# k=1\n",
        "# for i in range(1942,1970):\n",
        "#     pred_test['F'+str(k)]=lgb.predict(x_test[x_test['day']==(i)]) \n",
        "#     k+=1\n",
        "    \n",
        "# prediction_test = np.round(pred_test,2) \n",
        "\n",
        "# # Loop over each prediction day\n",
        "# # As rolling lags are the most timeconsuming\n",
        "# # we will calculate it for whole day\n",
        "# for PREDICT_DAY in range(1,29):    \n",
        "#     print('Predict | Day:', PREDICT_DAY)\n",
        "\n",
        "#     for store_id in STORES_IDS:\n",
        "        \n",
        "#         # Read all our models and make predictions\n",
        "#         # for each day/store pairs\n",
        "#         model_path = '/content/lgb_model_'+store_id+'.bin' \n",
        "        \n",
        "#         estimator = pickle.load(open(model_path, 'rb'))\n",
        "        \n",
        "#         day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
        "#         store_mask = base_test['store_id']==store_id\n",
        "        \n",
        "#         mask = (day_mask)&(store_mask)\n",
        "#         base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "    \n",
        "#     # Make good column naming and add \n",
        "#     # to all_preds DataFrame\n",
        "#     temp_df = base_test[day_mask][['id',TARGET]]\n",
        "#     temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "#     if 'id' in list(all_preds):\n",
        "#         all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "#     else:\n",
        "#         all_preds = temp_df.copy()\n",
        "        \n",
        "#     print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "#                   ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "#                   ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "#     del temp_df\n",
        "    \n",
        "# all_preds = all_preds.reset_index(drop=True)\n",
        "# all_preds"
      ],
      "metadata": {
        "id": "cGpScXGKpAzw"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}